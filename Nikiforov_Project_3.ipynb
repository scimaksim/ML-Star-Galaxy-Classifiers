{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3869e550-01cf-49c5-809b-4693f69c36bc",
   "metadata": {},
   "source": [
    "# Classifying stars and galaxies using machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96299d30-496a-4e24-8667-b047ac6809b9",
   "metadata": {},
   "source": [
    "Authored by Maksim Nikiforov\n",
    "\n",
    "NCSU ST590, Project 3\n",
    "\n",
    "Spring, 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e76693f0-cbba-44eb-9a33-2a4f5c938da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de7ce4d2-f6df-4517-ab98-62684ef17723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- objID: long (nullable = true)\n",
      " |-- ra: double (nullable = true)\n",
      " |-- dec: double (nullable = true)\n",
      " |-- specObjID: long (nullable = true)\n",
      " |-- psfMag_r: double (nullable = true)\n",
      " |-- modelMag_r: double (nullable = true)\n",
      " |-- petroMag_r: double (nullable = true)\n",
      " |-- fiberMag_r: double (nullable = true)\n",
      " |-- petroRad_r: double (nullable = true)\n",
      " |-- petroR50_r: double (nullable = true)\n",
      " |-- petroR90_r: double (nullable = true)\n",
      " |-- lnLStar_r: double (nullable = true)\n",
      " |-- lnLExp_r: double (nullable = true)\n",
      " |-- lnLDeV_r: double (nullable = true)\n",
      " |-- mE1_r: double (nullable = true)\n",
      " |-- mE2_r: double (nullable = true)\n",
      " |-- mRrCc_r: double (nullable = true)\n",
      " |-- type_r: integer (nullable = true)\n",
      " |-- type: integer (nullable = true)\n",
      " |-- specClass: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read CSV into a Spark data frame\n",
    "sdss_data = spark.read.options(header=\"True\", inferSchema='True',delimiter=',') \\\n",
    "  .csv(\"MyTable_mvnikifo.csv\")\n",
    "sdss_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fc08565-c204-4b69-91a8-76c554e825bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1030220"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdss_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bfeb78-cff3-41e5-a3c1-121b99430f04",
   "metadata": {},
   "source": [
    "There are missing values in this data, denoted by $0$ and $-9999$. These can be indicated more clearly with the designation \"None\". The number of missing values can be ascertained by converting the Spark DataFrame to a pandas-on-spark DataFrame and invoking the `.isnull().sum()` sequence of functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51e90bc2-eb65-48b5-b791-d8965aed7e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss_data = sdss_data.replace(-9999, None)\n",
    "sdss_data = sdss_data.replace(0, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8447ca-b11f-45be-aac8-1a1d4d5fb15a",
   "metadata": {},
   "source": [
    "There are nearly 12,000 rows with missing data. These can be removed to prepare the data for machine learning algorithms, leaving a total of 1,019,910 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a520e70f-89c2-4eff-8f49-8f1c83e55b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+---------+--------+----------+----------+----------+----------+----------+----------+---------+--------+--------+-----+-----+-------+------+----+---------+\n",
      "|objID| ra|dec|specObjID|psfMag_r|modelMag_r|petroMag_r|fiberMag_r|petroRad_r|petroR50_r|petroR90_r|lnLStar_r|lnLExp_r|lnLDeV_r|mE1_r|mE2_r|mRrCc_r|type_r|type|specClass|\n",
      "+-----+---+---+---------+--------+----------+----------+----------+----------+----------+----------+---------+--------+--------+-----+-----+-------+------+----+---------+\n",
      "|    0|  0|  0|        0|       0|         0|         0|         1|         0|        16|        16|       55|     104|      79|  524|  524|    524|     0|   0|     9989|\n",
      "+-----+---+---+---------+--------+----------+----------+----------+----------+----------+----------+---------+--------+--------+-----+-----+-------+------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count total number of missing values\n",
    "# Based on example from https://sparkbyexamples.com/pyspark/pyspark-find-count-of-null-none-nan-values/\n",
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "\n",
    "sdss_data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in sdss_data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd7e926b-6a0b-4ab5-8d0e-585c9fe66459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1019910"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "sdss_data = sdss_data.dropna()\n",
    "sdss_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c241a4d-1461-4bd9-863f-bd99a56e38c0",
   "metadata": {},
   "source": [
    "The data also contains a `specClass` column with values that correspond to the following classifications (https://skyserver.sdss.org/dr7/en/help/browser/enum.asp?n=SpecClass): \n",
    "\n",
    "| Name      | Value | Description                                                                                      |\n",
    "|-----------|-------|--------------------------------------------------------------------------------------------------|\n",
    "| UNKNOWN   |   0   | Spectrum not classifiable (zConf < 0.25).                                                        |\n",
    "| STAR      |   1   | Spectrum of a star.                                                                              |\n",
    "| GALAXY    |   2   | Spectrum of a galaxy.                                                                            |\n",
    "| QSO       |   3   | Spectrum of a quasi-stellar object.                                                              |\n",
    "| HIZ_QSO   |   4   | Spectrum of a high-redshift quasar (z>2.3), whose redshift is confirmed by a Ly-alpha estimator. |\n",
    "| SKY       |   5   | Spectrum of blank sky.                                                                           |\n",
    "| STAR_LATE |   6   | Star dominated bt molecular bands M or later.                                                    |\n",
    "| GAL_EM    |   7   | Emission line galaxy (placeholder).                                                              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884a9c7f-b45a-4760-b4e9-6d45c8cc9429",
   "metadata": {},
   "source": [
    "The intent of this project is to classify only stars and galaxies, and all other observations should be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f112842d-5509-4df7-820b-7d1b18be9d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss_data = sdss_data.filter((sdss_data.specClass == 1) | \\\n",
    "                              (sdss_data.specClass == 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603e34db-3031-4d68-9965-7e3d8d29f762",
   "metadata": {},
   "source": [
    "At this point, the data contains observations for 81,633 stars and 802,474 galaxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92e0d1ee-5bc1-4199-9862-d7514b16d57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|type| count|\n",
      "+----+------+\n",
      "|   6| 81633|\n",
      "|   3|802474|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdss_data.groupBy(\"type\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f900d6-9720-4003-bd78-13c014574f58",
   "metadata": {},
   "source": [
    "## Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63e9a8a4-fe20-4cb2-85be-b05bc29269c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "sqlTrans = SQLTransformer(\n",
    "    statement = \"SELECT psfMag_r, modelMag_r, petroMag_r, fiberMag_r, petroRad_r, petroR50_r, petroR90_r, lnLStar_r, lnLExp_r, lnLDeV_r, mE1_r, mE2_r, mRrCc_r, type as label FROM __THIS__\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e34a7bb2-d89e-4914-a755-14611600e5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+----------+----------+----------+----------+---------+---------+---------+----------+-----------+--------+-----+\n",
      "|psfMag_r|modelMag_r|petroMag_r|fiberMag_r|petroRad_r|petroR50_r|petroR90_r|lnLStar_r| lnLExp_r| lnLDeV_r|     mE1_r|      mE2_r| mRrCc_r|label|\n",
      "+--------+----------+----------+----------+----------+----------+----------+---------+---------+---------+----------+-----------+--------+-----+\n",
      "|18.01274|  16.43724|  16.61505|  18.07128|  5.936419|  2.581169|   7.24652|-15946.72|-1429.818|-776.5237|-0.4130035|  0.5511445| 37.7417|    3|\n",
      "| 17.7713|    16.704|  16.79972|  17.86816|  3.742817|  1.750927|  5.314383|-10572.39|-1694.458|-133.5919| 0.0779022| -0.1298255|12.46036|    3|\n",
      "|18.71541|  17.15762|  17.09714|  18.76385|  6.089326|  2.881765|  6.535208|-7992.962|-337.2565|-407.3528|0.03209908|  0.4520678|43.33707|    3|\n",
      "|17.59361|  16.33401|  16.40417|  17.73234|  5.358847|  2.365247|  7.632291|-15323.06| -3282.95|-114.9684|  0.082524|-0.03256264|13.30449|    3|\n",
      "|18.77835|  17.14057|  17.37658|  18.80704|  5.627723|  2.461274|  6.215099|-6404.534|-516.8677|-103.3293|0.09326434|   0.456513| 23.6239|    3|\n",
      "+--------+----------+----------+----------+----------+----------+----------+---------+---------+---------+----------+-----------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlTrans.transform(sdss_data).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba24dd4-c9f0-4b77-99ed-68526ff94090",
   "metadata": {},
   "source": [
    "The data set should be split into a training and a testing test before applying transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f0c0455-b7d1-4c74-8e54-8104f4aaaefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "707626 176481\n"
     ]
    }
   ],
   "source": [
    "train_SDSS, test_SDSS = sqlTrans.transform(sdss_data).randomSplit([0.8,0.2], seed = 1)\n",
    "print(train_SDSS.count(), test_SDSS.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3bfad018-6647-4a3c-adfc-24590b777542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['psfMag_r',\n",
       " 'modelMag_r',\n",
       " 'petroMag_r',\n",
       " 'fiberMag_r',\n",
       " 'petroRad_r',\n",
       " 'petroR50_r',\n",
       " 'petroR90_r',\n",
       " 'lnLStar_r',\n",
       " 'lnLExp_r',\n",
       " 'lnLDeV_r',\n",
       " 'mE1_r',\n",
       " 'mE2_r',\n",
       " 'mRrCc_r']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our list of features.\n",
    "# Obtain all column names (list)\n",
    "features_list = train_SDSS.columns\n",
    "\n",
    "# Specify the column names to remove\n",
    "remove_col = [\"label\"]\n",
    "\n",
    "# Remove these columns using a list comprehension loop and output final list of features\n",
    "features_list = [col_name for col_name in features_list if col_name not in remove_col]\n",
    "features_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c015a1-a382-43a0-9d13-32ae71bcb309",
   "metadata": {},
   "source": [
    "Since the data is now split, it can be transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "03d6b347-9711-43cd-88f7-e6ef80413faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      scaledFeatures|\n",
      "+--------------------+\n",
      "|[15.6929391950376...|\n",
      "|[15.9325044686517...|\n",
      "|[15.9685972696521...|\n",
      "|[15.9804840538833...|\n",
      "|[16.0223871859818...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=features_list, outputCol=\"unscaledFeatures\")\n",
    "\n",
    "features_df = vecAssembler.transform(train_SDSS)\n",
    "\n",
    "stdScaler = StandardScaler(inputCol=\"unscaledFeatures\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "scaled_df = stdScaler.fit(features_df).transform(features_df)\n",
    "\n",
    "\n",
    "scaled_df.select(\"scaledFeatures\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "25117d9c-e014-4c23-b018-eef5d8514399",
   "metadata": {},
   "outputs": [],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=features_list, outputCol=\"unscaledFeatures\")\n",
    "\n",
    "features_df_test = vecAssembler.transform(test_SDSS)\n",
    "\n",
    "stdScaler = StandardScaler(inputCol=\"unscaledFeatures\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "scaled_df_test = stdScaler.fit(features_df_test).transform(features_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "712b4397-75fa-4d74-86c8-0ef1b075e9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(labelCol=\"label\", featuresCol=\"scaledFeatures\")\n",
    "\n",
    "model = rfc.fit(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0c8f450a-ad56-4f5c-a6f0-016d65e7d2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+----------+----------+----------+----------+---------+---------+---------+-----------+------------+--------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|psfMag_r|modelMag_r|petroMag_r|fiberMag_r|petroRad_r|petroR50_r|petroR90_r|lnLStar_r| lnLExp_r| lnLDeV_r|      mE1_r|       mE2_r| mRrCc_r|label|    unscaledFeatures|      scaledFeatures|       rawPrediction|         probability|prediction|\n",
      "+--------+----------+----------+----------+----------+----------+----------+---------+---------+---------+-----------+------------+--------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|14.42715|   14.4284|   14.4905|  14.76792|  1.134293| 0.5847612|  1.286333|-256.9894|-260.3972| -259.931| 0.02392568|  -0.0334613|2.521054|    6|[14.42715,14.4284...|[16.0016856109306...|[0.0,0.0,0.0,1.13...|[0.0,0.0,0.0,0.05...|       6.0|\n",
      "|14.75226|  15.09743|  14.80867|  15.36704|  1.778126| 0.9739496|  2.130796|-640.2658|-315.4142|-129.3483| -0.2692961|  0.04587317|9.451305|    6|[14.75226,15.0974...|[16.3622771351727...|[0.0,0.0,0.0,7.38...|[0.0,0.0,0.0,0.36...|       6.0|\n",
      "|14.79217|  15.31072|  14.83283|  15.62224|  2.427231|  1.291683|  2.824174|-899.4579|-319.8453|-71.44184| 0.02639799|  0.05775896|11.65881|    6|[14.79217,15.3107...|[16.4065427921273...|[0.0,0.0,0.0,14.5...|[0.0,0.0,0.0,0.72...|       3.0|\n",
      "|14.84014|  14.80977|  14.85198|  15.20753|  1.945631| 0.9685486|  2.131466|-356.0252|-254.6996|  -249.96|-0.09935738| -0.07640104|7.268594|    6|[14.84014,14.8097...|[16.4597480931574...|[0.0,0.0,0.0,11.9...|[0.0,0.0,0.0,0.59...|       3.0|\n",
      "| 14.8451|  14.83546|  14.87971|  15.17298|  1.429334| 0.7335632|   1.58022|-396.9622|-372.7509|-372.8664| -0.1852526|-0.004246251|4.376443|    6|[14.8451,14.83546...|[16.4652494125885...|[0.0,0.0,0.0,7.51...|[0.0,0.0,0.0,0.37...|       6.0|\n",
      "+--------+----------+----------+----------+----------+----------+----------+---------+---------+---------+-----------+------------+--------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = model.transform(scaled_df_test)\n",
    "pred.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d8959e7-7a79-45a4-b670-284e1895a743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  0.9951260086324447\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator=MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "acc = evaluator.evaluate(pred)\n",
    "print(\"Prediction Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80bae3ca-8737-4bf0-b16b-e393d2e5224b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[159693    380]\n",
      " [   479  15929]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred=pred.select(\"prediction\").collect()\n",
    "y_orig=pred.select(\"label\").collect()\n",
    "\n",
    "cm = confusion_matrix(y_orig, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b0ebf167-40d2-4fd7-bdd2-759d6352e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "faint_mag = pred.filter((pred.modelMag_r >= 20.5) & (pred.modelMag_r <= 21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "506c752d-0a7a-4c52-8c80-6a5f29e3e432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  0.9223043524303716\n"
     ]
    }
   ],
   "source": [
    "acc_faint = evaluator.evaluate(faint_mag)\n",
    "print(\"Prediction Accuracy: \", acc_faint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54acf4ec-7007-4819-b8d9-6bb8fec1c700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages = [vecAssembler, assembler, lr])"
   ]
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
