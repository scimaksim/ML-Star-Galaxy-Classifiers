{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3869e550-01cf-49c5-809b-4693f69c36bc",
   "metadata": {},
   "source": [
    "# Classifying stars and galaxies using machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96299d30-496a-4e24-8667-b047ac6809b9",
   "metadata": {},
   "source": [
    "Authored by Maksim Nikiforov\n",
    "\n",
    "NCSU ST590, Project 3\n",
    "\n",
    "Spring, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa22a62-02fb-430e-8784-ab471ca531ee",
   "metadata": {},
   "source": [
    "## Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e76693f0-cbba-44eb-9a33-2a4f5c938da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de7ce4d2-f6df-4517-ab98-62684ef17723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- objID: long (nullable = true)\n",
      " |-- ra: double (nullable = true)\n",
      " |-- dec: double (nullable = true)\n",
      " |-- specObjID: long (nullable = true)\n",
      " |-- psfMag_r: double (nullable = true)\n",
      " |-- modelMag_r: double (nullable = true)\n",
      " |-- petroMag_r: double (nullable = true)\n",
      " |-- fiberMag_r: double (nullable = true)\n",
      " |-- petroRad_r: double (nullable = true)\n",
      " |-- petroR50_r: double (nullable = true)\n",
      " |-- petroR90_r: double (nullable = true)\n",
      " |-- lnLStar_r: double (nullable = true)\n",
      " |-- lnLExp_r: double (nullable = true)\n",
      " |-- lnLDeV_r: double (nullable = true)\n",
      " |-- mE1_r: double (nullable = true)\n",
      " |-- mE2_r: double (nullable = true)\n",
      " |-- mRrCc_r: double (nullable = true)\n",
      " |-- type_r: integer (nullable = true)\n",
      " |-- type: integer (nullable = true)\n",
      " |-- specClass: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read CSV into a Spark data frame\n",
    "sdss_data = spark.read.options(header=\"True\", \n",
    "                               inferSchema='True',\n",
    "                               delimiter=',').csv(\"sdss_test_mvnikifo.csv\")\n",
    "sdss_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3fc08565-c204-4b69-91a8-76c554e825bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdss_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d2d3dc-64ed-4376-ae46-46044cb1b471",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bfeb78-cff3-41e5-a3c1-121b99430f04",
   "metadata": {},
   "source": [
    "There are missing values in this data, denoted by $0$ and $-9999$. These can be indicated more clearly with the designation \"None\". The number of missing values can be ascertained by converting the Spark DataFrame to a pandas-on-spark DataFrame and invoking the `.isnull().sum()` sequence of functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51e90bc2-eb65-48b5-b791-d8965aed7e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss_data = sdss_data.replace(-9999, None)\n",
    "sdss_data = sdss_data.replace(0, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8447ca-b11f-45be-aac8-1a1d4d5fb15a",
   "metadata": {},
   "source": [
    "There are nearly 12,000 rows with missing data. These can be removed to prepare the data for machine learning algorithms, leaving a total of 1,019,910 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a520e70f-89c2-4eff-8f49-8f1c83e55b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+---------+--------+----------+----------+----------+----------+----------+----------+---------+--------+--------+-----+-----+-------+------+----+---------+\n",
      "|objID| ra|dec|specObjID|psfMag_r|modelMag_r|petroMag_r|fiberMag_r|petroRad_r|petroR50_r|petroR90_r|lnLStar_r|lnLExp_r|lnLDeV_r|mE1_r|mE2_r|mRrCc_r|type_r|type|specClass|\n",
      "+-----+---+---+---------+--------+----------+----------+----------+----------+----------+----------+---------+--------+--------+-----+-----+-------+------+----+---------+\n",
      "|    0|  0|  0|        0|       0|         0|         0|         1|         0|         1|         1|       11|      18|      17|   29|   29|     29|     0|   0|     1180|\n",
      "+-----+---+---+---------+--------+----------+----------+----------+----------+----------+----------+---------+--------+--------+-----+-----+-------+------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count total number of missing values\n",
    "# Based on example from https://sparkbyexamples.com/pyspark/pyspark-find-count-of-null-none-nan-values/\n",
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "\n",
    "sdss_data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in sdss_data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd7e926b-6a0b-4ab5-8d0e-585c9fe66459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98781"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove rows with missing values and calculate new row count\n",
    "sdss_data = sdss_data.dropna()\n",
    "sdss_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c241a4d-1461-4bd9-863f-bd99a56e38c0",
   "metadata": {},
   "source": [
    "The data also contains a `specClass` column with values that correspond to the following classifications (https://skyserver.sdss.org/dr7/en/help/browser/enum.asp?n=SpecClass): \n",
    "\n",
    "| Name      | Value | Description                                                                                      |\n",
    "|-----------|-------|--------------------------------------------------------------------------------------------------|\n",
    "| UNKNOWN   |   0   | Spectrum not classifiable (zConf < 0.25).                                                        |\n",
    "| STAR      |   1   | Spectrum of a star.                                                                              |\n",
    "| GALAXY    |   2   | Spectrum of a galaxy.                                                                            |\n",
    "| QSO       |   3   | Spectrum of a quasi-stellar object.                                                              |\n",
    "| HIZ_QSO   |   4   | Spectrum of a high-redshift quasar (z>2.3), whose redshift is confirmed by a Ly-alpha estimator. |\n",
    "| SKY       |   5   | Spectrum of blank sky.                                                                           |\n",
    "| STAR_LATE |   6   | Star dominated bt molecular bands M or later.                                                    |\n",
    "| GAL_EM    |   7   | Emission line galaxy (placeholder).                                                              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884a9c7f-b45a-4760-b4e9-6d45c8cc9429",
   "metadata": {},
   "source": [
    "The intent of this project is to classify only stars and galaxies, and all other observations should be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f112842d-5509-4df7-820b-7d1b18be9d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss_data = sdss_data.filter((sdss_data.specClass == 1) | \\\n",
    "                              (sdss_data.specClass == 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603e34db-3031-4d68-9965-7e3d8d29f762",
   "metadata": {},
   "source": [
    "At this point, the data contains observations for 81,633 stars and 802,474 galaxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92e0d1ee-5bc1-4199-9862-d7514b16d57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|type|count|\n",
      "+----+-----+\n",
      "|   6| 8589|\n",
      "|   3|77299|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdss_data.groupBy(\"type\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba24dd4-c9f0-4b77-99ed-68526ff94090",
   "metadata": {},
   "source": [
    "The data set should be split into a training and a testing test before applying transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5f0c0455-b7d1-4c74-8e54-8104f4aaaefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68513 17375\n"
     ]
    }
   ],
   "source": [
    "train, test = sdss_data.randomSplit([0.8,0.2])\n",
    "print(train.count(), test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2caea67-3a1b-4bb9-a37f-a186d72ef819",
   "metadata": {},
   "source": [
    "## Machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f900d6-9720-4003-bd78-13c014574f58",
   "metadata": {},
   "source": [
    "### Random forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed1ec0-9887-41d5-bc26-e3b3dce4e4f0",
   "metadata": {},
   "source": [
    "#### Set up transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "63e9a8a4-fe20-4cb2-85be-b05bc29269c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "sqlTrans = SQLTransformer(\n",
    "    statement = \"SELECT psfMag_r, modelMag_r, petroMag_r, fiberMag_r, \\\n",
    "                        petroRad_r, petroR50_r, petroR90_r, lnLStar_r, \\\n",
    "                        lnLExp_r, lnLDeV_r, mE1_r, mE2_r, mRrCc_r, \\\n",
    "                        type as label FROM __THIS__\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3bfad018-6647-4a3c-adfc-24590b777542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['psfMag_r',\n",
       " 'modelMag_r',\n",
       " 'petroMag_r',\n",
       " 'fiberMag_r',\n",
       " 'petroRad_r',\n",
       " 'petroR50_r',\n",
       " 'petroR90_r',\n",
       " 'lnLStar_r',\n",
       " 'lnLExp_r',\n",
       " 'lnLDeV_r',\n",
       " 'mE1_r',\n",
       " 'mE2_r',\n",
       " 'mRrCc_r']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our list of features by dropping unused columns\n",
    "features_list = sqlTrans.transform(train).drop(\"label\").columns\n",
    "features_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c015a1-a382-43a0-9d13-32ae71bcb309",
   "metadata": {},
   "source": [
    "We can set up transformations for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03d6b347-9711-43cd-88f7-e6ef80413faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features_list, outputCol=\"features\")\n",
    "\n",
    "#scaler = StandardScaler(inputCol=\"unscaledFeatures\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdf3379-f5e3-4884-9566-a49bf6b8b9cb",
   "metadata": {},
   "source": [
    "We can select an algorithm for our data and instantiate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "712b4397-75fa-4d74-86c8-0ef1b075e9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4607de70-0652-4446-9978-981e74c13d3d",
   "metadata": {},
   "source": [
    "Finally, we can set up a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5271121c-efd8-4b3c-a82f-9bdd208db6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages = [sqlTrans, assembler, rfc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a25947-b932-45f8-a888-3aa6df95c8c2",
   "metadata": {},
   "source": [
    "We can then set up cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d91aeaf1-bfb6-448f-ac80-245ac698f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "rfcParamGrid = (ParamGridBuilder()\n",
    "             .addGrid(rfc.maxDepth, [0, 10, 20, 30])\n",
    "             .addGrid(rfc.numTrees, [5, 10, 20, 30, 40])\n",
    "             .build())\n",
    "\n",
    "crossVal = CrossValidator(estimator = pipeline,\n",
    "                          estimatorParamMaps = rfcParamGrid,\n",
    "                          evaluator = MulticlassClassificationEvaluator(),\n",
    "                          numFolds = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "59d25a0c-e0c0-47f7-aa2d-7197c691f94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Run cross-validation, and choose the best set of parameters\n",
    "\n",
    "cvModel = crossVal.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9034ca34-5b15-4d59-819a-a6e0beec8cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfcPredictions = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6769ae1a-f6db-4bdd-9a0d-ebe35d1c6312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9960181108638269\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', MulticlassClassificationEvaluator().evaluate(rfcPredictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "80bae3ca-8737-4bf0-b16b-e393d2e5224b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[15603    24]\n",
      " [   45  1703]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred=rfcPredictions.select(\"prediction\").collect()\n",
    "y_orig=rfcPredictions.select(\"label\").collect()\n",
    "\n",
    "cm = confusion_matrix(y_orig, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1c288d-e5bf-4c9c-a672-37e8e32c6abe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab9d1d60-a111-44bc-957f-ecc3d9ee1330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "grid_lr = ParamGridBuilder().addGrid(lr.maxIter, [0, 1]).build()\n",
    "evaluator_lr = MulticlassClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c778ce5e-23c0-4675-a671-0464f0946e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr = Pipeline(stages = [sqlTrans, assembler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d43b0ec4-3b50-4b00-b889-0e7083e44b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/25 22:02:24 WARN CacheManager: Asked to cache already cached data.\n",
      "22/04/25 22:02:24 WARN CacheManager: Asked to cache already cached data.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cv = CrossValidator(estimator=pipeline_lr, estimatorParamMaps=grid_lr, evaluator=evaluator_lr,\n",
    "    parallelism=2)\n",
    "cvModel_lr = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4365ac46-7991-4aa1-896b-fe968560d4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 461:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.940197424404984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_error = MulticlassClassificationEvaluator().evaluate(cvModel_lr.transform(test))\n",
    "print(test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f5fb5a-ef4c-4917-a622-7af7a28585bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75065a5a-a391-47bb-8107-bf0901d37a81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d8959e7-7a79-45a4-b670-284e1895a743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  0.9951260086324447\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator=MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "acc = evaluator.evaluate(pred)\n",
    "print(\"Prediction Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b0ebf167-40d2-4fd7-bdd2-759d6352e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "faint_mag = pred.filter((pred.modelMag_r >= 20.5) & (pred.modelMag_r <= 21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "506c752d-0a7a-4c52-8c80-6a5f29e3e432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  0.9223043524303716\n"
     ]
    }
   ],
   "source": [
    "acc_faint = evaluator.evaluate(faint_mag)\n",
    "print(\"Prediction Accuracy: \", acc_faint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69eab3b0-5606-40fd-93a6-6abc18ca8dfc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BinaryClassificationEvaluator\n\u001b[0;32m      3\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m BinaryClassificationEvaluator(\n\u001b[0;32m      4\u001b[0m     labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m,                     \n\u001b[0;32m      5\u001b[0m     rawPredictionCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m,       \n\u001b[0;32m      6\u001b[0m     metricName\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mareaUnderROC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 9\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mevaluate(\u001b[43mpred\u001b[49m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArea under ROC = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pred' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    " \n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\",                     \n",
    "    rawPredictionCol=\"prediction\",       \n",
    "    metricName=\"areaUnderROC\",\n",
    ")\n",
    " \n",
    "accuracy = evaluator.evaluate(pred)\n",
    "print(f\"Area under ROC = {accuracy} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb29eac-28bf-4adb-b1b6-2530e9c52484",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e18e1ab9-7ecb-4b75-a0c7-c7a9aee673c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dtClassifier = DecisionTreeClassifier()\n",
    "\n",
    "dtPipeline = Pipeline().setStages([sqlTrans, assembler, dtClassifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e44ef681-b2a9-4dde-ac13-1ae1a68e5904",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtParamGrid = (ParamGridBuilder()\n",
    "             .addGrid(dtClassifier.maxDepth, [0, 5, 10, 20]) \\\n",
    "             .build())\n",
    "\n",
    "dtCrossVal = CrossValidator(estimator = dtPipeline,\n",
    "                          estimatorParamMaps = dtParamGrid,\n",
    "                          evaluator = MulticlassClassificationEvaluator(),\n",
    "                          numFolds = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "843ac385-fc6d-4155-b414-b4adfcae503d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Run cross-validation, and choose the best set of parameters\n",
    "\n",
    "dtCVModel = dtCrossVal.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6c005660-12ac-4245-bf6b-4b92cef8d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtPredictions = dtCVModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9294eca-450b-4b99-b818-d6b6ee921670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
